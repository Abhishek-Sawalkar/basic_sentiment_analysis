{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic_BERT_Model_(2) (1).ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNUlJ2HkEAS-",
        "colab_type": "text"
      },
      "source": [
        "# BERT "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLZm_9h7gX2T",
        "colab_type": "text"
      },
      "source": [
        "### Connect colab with drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a35kENAxjYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "\n",
        "#IMPORT FILES FROM DRIVE INTO GOOGLE-COLAB:\n",
        "\n",
        "#STEP-1: Import Libraries\n",
        "\n",
        "# Code to read csv file into colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "#STEP-2: Autheticate E-Mail ID\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#STEP-3: Get File from Drive using file-ID\n",
        "\n",
        "#2.1 Get the file\n",
        "downloaded = drive.CreateFile({'id':'1UEl2hUf4t8iuHUf_SE7ExlCeWyLS0_Wu'}) # replace the id with id of file you want to linkable link and delete from 'https....id='\n",
        "downloaded.GetContentFile('ibm_0k-60k.csv')  # file name to be imported to colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHAb4wW3EHHe",
        "colab_type": "text"
      },
      "source": [
        "## 1. Check Device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH_iPhh9C3a4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9sOD00mFDEM",
        "colab_type": "text"
      },
      "source": [
        "## 2. Install pytorch interface for bert model and required libraries\n",
        "At the moment, the Hugging Face library seems to be the most widely accepted and powerful pytorch interface for working with BERT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD6DgnEwEoT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! pip install pytorch-pretrained-bert pytorch-nlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G23HuR4xOg7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovsdp_owFdiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers import BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "torch.cuda.empty_cache()\n",
        "import transformers\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "% matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZb-caSWRoDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_val = 42\n",
        "\n",
        "# # Set the seed value all over the place to make this reproducible.\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvAAaxtaxs5k",
        "colab_type": "text"
      },
      "source": [
        "In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDcIhxpYwwbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y81bQADlzHJt",
        "colab_type": "text"
      },
      "source": [
        "## 3. Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzKmlZCs2ygK",
        "colab_type": "text"
      },
      "source": [
        "### Upload file\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlMzUhgoxjBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df = pd.read_csv(\"in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "df = pd.read_csv(\"ibm_0k-60k.csv\")\n",
        "df = pd.DataFrame(df)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD4sFtxw3ZTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df = df.sample(1000)\n",
        "\n",
        "df.Text = df.Text.astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYvNKW1_rYv3",
        "colab_type": "text"
      },
      "source": [
        "Our target variable is 'ibm_sent' which contain 7 sentiments. So this is  a multiclass classification problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-L6zJ4TqAif",
        "colab_type": "text"
      },
      "source": [
        "### Converting labels to proper format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDwWteKWSg47",
        "colab_type": "text"
      },
      "source": [
        "Code for Label encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONODabvx_v9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.ibm_sent = df.ibm_sent.astype(str)\n",
        "df[\"ibm_sent\"]= df[\"ibm_sent\"].replace(\"nan\", \"Neutral\") \n",
        "t = df.ibm_sent.unique()\n",
        "print(t)\n",
        "\n",
        "def la(sent):\n",
        "  if sent == \"Sadness\":\n",
        "    return 0\n",
        "  if sent == \"Confident\":\n",
        "    return 1\n",
        "  if sent == \"Neutral\":\n",
        "    return 2\n",
        "  if sent == \"Joy\":\n",
        "    return 3\n",
        "  if sent == \"Analytical\":\n",
        "    return 4\n",
        "  if sent == \"Anger\":\n",
        "    return 5\n",
        "  if sent == \"Fear\":\n",
        "    return 6\n",
        "\n",
        "df['ibm_sent_cat'] = df['ibm_sent'].apply(la)\n",
        "labels = df['ibm_sent_cat'].values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN-SjDOYSksZ",
        "colab_type": "text"
      },
      "source": [
        "Code for converting the labels to one hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m0HM6NbQpjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(sent):\n",
        "  temp = []\n",
        "  for i in range(0,7):\n",
        "    if sent == i:\n",
        "      temp.append(1)\n",
        "    else:\n",
        "      temp.append(0)\n",
        "  return temp\n",
        "\n",
        "df['one_hot_sent'] = df['ibm_sent_cat'].apply(one_hot)\n",
        "df['one_hot_sent'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew2VBCSxiwUR",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_GLx6FV1cdD",
        "colab_type": "text"
      },
      "source": [
        "### configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aB_3MPc1b1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 128\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_batch_size = 32\n",
        "test_batch_size = 32\n",
        "epoches = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNg-3nnBr42i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomDataset:\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.comment_text = dataframe.Text.values\n",
        "        self.targets = dataframe.one_hot_sent.values\n",
        "        # print(self.targets)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.comment_text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        comment_text = str(self.comment_text[index])\n",
        "        comment_text = \" \".join(comment_text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            comment_text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgZsaZYDqH08",
        "colab_type": "text"
      },
      "source": [
        "### Data split into validation and train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oamLhlBqM44F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the dataset and dataloader for the neural network\n",
        "\n",
        "train_size = 0.8\n",
        "train_dataset=df.sample(frac=train_size,random_state=200)\n",
        "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = CustomDataset(df,tokenizer, MAX_LEN)\n",
        "testing_set = CustomDataset(df, tokenizer, MAX_LEN)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqwLf7zm1XnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_params = {'batch_size' : train_batch_size,\n",
        "                 'shuffle' : True,\n",
        "                 'num_workers' : 0}\n",
        "test_params = {'batch_size' : test_batch_size,\n",
        "                 'shuffle' : True,\n",
        "                 'num_workers' : 0}\n",
        "\n",
        "train_loader = DataLoader(training_set, **train_params)\n",
        "test_loader = DataLoader(testing_set, **test_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJdRZxppSq3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xExJNmzHSgwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for step, data in enumerate(train_loader):\n",
        "  print(data['ids'])\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60SV2BdySTUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['ids'].size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CAijRu2upin",
        "colab_type": "text"
      },
      "source": [
        "## BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xfetc9t1vm60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc \n",
        "\n",
        "# Your code with pytorch using GPU\n",
        "\n",
        "gc.collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJzpzDbC_CnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTclass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTclass, self).__init__()\n",
        "        # self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "        # self.l2 = torch.nn.Dropout(0.3)\n",
        "        # self.l3 = torch.nn.Linear(768, 7)\n",
        "        self.model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 7)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids, labels):\n",
        "        # _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
        "        # output_2 = self.l2(output_1)\n",
        "        # output = self.l3(output_2)\n",
        "        output = self.model(input_ids = ids, attention_mask= mask)\n",
        "        return output\n",
        "\n",
        "\n",
        "model = BERTclass()\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szASmR8tt51g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# del model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gsm_ohG1Jlgs",
        "colab_type": "text"
      },
      "source": [
        "Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n",
        "\n",
        "For the purposes of fine-tuning, the authors recommend the following hyperparameter ranges:\n",
        "- Batch size: 16, 32\n",
        "- Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "- Number of epochs: 2, 3, 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ioVITWD06aa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD1A1F4ZvoWQ",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer & Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTTLLGufnp9k",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLYvNudY5ZjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = transformers.AdamW(optimizer_grouped_parameters,\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNaKMon-n6FN",
        "colab_type": "text"
      },
      "source": [
        "Helper function for formatting elapsed times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ8iidkobo2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSqi7I86nf9Z",
        "colab_type": "text"
      },
      "source": [
        "### Accuracy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5enz51lcKUI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def acc(preds, labels):\n",
        "  pred_flat = np.argmax(preds, axis=1) # np.argmax - Returns the indices of the maximum values along an axis.\n",
        "  labels_flat = labels.flatten() # Flatten - Return a copy of the array collapsed into one dimension.\n",
        "  return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iotwBOJvn8Wg",
        "colab_type": "text"
      },
      "source": [
        "### Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8usJi1KbARGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(training_set) * epoches\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCZ23UpCH4VK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "seed_val = 42\n",
        "\n",
        "# # Set the seed value all over the place to make this reproducible.\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOmhiOkUPur1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(outputs, targets):\n",
        "  # print(f'o/p : {outputs}, targets  : {targets.size()}')\n",
        "  return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfDmoc5VIRMv",
        "colab_type": "text"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcpR09YUHWB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_fn(epoch):\n",
        "    for step, data in enumerate(train_loader, 0):\n",
        "\n",
        "        ids = data['ids'].to(device)\n",
        "        mask = data['mask'].to(device)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(ids, mask, token_type_ids, labels=targets)\n",
        "        \n",
        "        # outputs = outputs[0].detach().cpu()\n",
        "        # targets = targets.detach().cpu()\n",
        "        loss = loss_fn(outputs[0], targets)\n",
        "        \n",
        "        if step%100 ==0 and not step != 0:\n",
        "            print(f'epoch : {epoch}    loss : {loss}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "def val_fn(epoch):\n",
        "    model.eval()\n",
        "    fin_targets=[]\n",
        "    fin_outputs=[]\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "    return fin_outputs, fin_targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDAh7qC2QwmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train\n",
        "for ep in range(epoches):\n",
        "    train_fn(ep)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DWBEPuYUQDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# validation\n",
        "for epoch in range(EPOCHS):\n",
        "    outputs, targets = validation(epoch)\n",
        "    outputs = np.array(outputs) >= 0.5\n",
        "    accuracy = metrics.accuracy_score(targets, outputs)\n",
        "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
        "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
        "    print(f\"Accuracy Score = {accuracy}\")\n",
        "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
        "    print(f\"F1 Score (Macro) = {f1_score_macro}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJFKo54VebQq",
        "colab_type": "text"
      },
      "source": [
        "## Saving & Loading Fine-Tuned Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzrd-Ui9fP01",
        "colab_type": "text"
      },
      "source": [
        "### To colab itself"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJDxfGp4d1Q_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG2iLtlefTES",
        "colab_type": "text"
      },
      "source": [
        "### To Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRVXy9W5d1Wb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "dr = \"drive/My Drive/BERT/\"\n",
        "\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(dr)\n",
        "tokenizer.save_pretrained(dr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKqhsSUkfjsD",
        "colab_type": "text"
      },
      "source": [
        "### Load Model\n",
        "\n",
        "The following functions will load the model back from disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQHxgYbUd1Ue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load a trained model and vocabulary that you have fine-tuned\n",
        "model = model_class.from_pretrained(output_dir)\n",
        "tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
        "\n",
        "# Copy the model to the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}